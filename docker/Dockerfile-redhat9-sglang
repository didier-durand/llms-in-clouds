#hadolint ignore=DL3007
FROM redhat/ubi9:latest
ARG CUDA_REPO="https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo"

ARG REDHAT_USERNAME
ARG REDHAT_PASSWORD

# check if populated
RUN echo "auth: ${REDHAT_USERNAME} / ${REDHAT_PASSWORD}"

# register to Redhat for package download
RUN echo "auth: ${REDHAT_USERNAME} / ${REDHAT_PASSWORD}" \
    && subscription-manager register --username ${REDHAT_USERNAME} --password ${REDHAT_PASSWORD}  \
    && echo "Redhat registration successful!"  \
    && subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms  \
    && subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms  \
    && subscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms \
    && echo "Additional repositories enabled!"

# to extend / customize SGLang at build time
ARG CUSTOMIZE_SGLANG="customize_sglang.sh"
# to extend / customize SGLang launch steps and parameters
ARG START_SGLANG="start_sglang.sh"
# model dir must be created at image build time to allow volume bind mounts on container start
ARG SGL_MODEL_DIR="/home/model"
# to create directory for accessing model weights via local storage (Docker volume)
WORKDIR ${SGL_MODEL_DIR}
# to create directory for SGLang extensions
WORKDIR "/home/sglang"

# versions of components
ARG CUDA_VERSION_MAJOR="12"
ARG CUDA_VERSION_DASH="12-6"
ARG CUDA_VERSION_DOT="12.6"
ARG PYTHON_VERSION="3.12"
ARG TORCH_VERSION="2.6"
ARG SGL_VERSION="0.4.8.post1"

ARG SGL_TP_SIZE=2

ARG SGL_HOST="0.0.0.0"
ARG SGL_PORT=30000

 # debug, info, warning, error
ARG SGL_LOG_LEVEL="info"

# install headers, tools & utilities (kernel-devel required by CUDA graph compiler)
# hadolint ignore=DL3041
RUN dnf update -y \
    && dnf install -y wget findutils which grep sed git patch zlib ca-certificates \
    && dnf install -y kernel-headers kernel-devel \
    && dnf clean all

# package dkms from EPEL is required by nvidia-fs-dkms
# hadolint ignore=DL3041
RUN dnf config-manager --add-repo ${CUDA_REPO}  \
    && dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm  \
    && dnf install -y cuda-toolkit-${CUDA_VERSION_DASH}  \
    && dnf install -y nvidia-gds-${CUDA_VERSION_DASH}  \
    && dnf install -y --allowerasing cudnn9-cuda-${CUDA_VERSION_MAJOR}  \
    && dnf install -y libcusparselt0 libcusparselt-devel \
    && dnf install -y libnccl libnccl-devel libnccl-static \
    && dnf clean all

ENV PATH=${PATH}:/usr/local/cuda-${CUDA_VERSION_DOT}/bin
ENV LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-${CUDA_VERSION_DOT}/lib64

# install Python & sglang
# hadolint ignore=DL3041
RUN dnf install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-pip python${PYTHON_VERSION}-devel \
    && dnf clean all \
    && python${PYTHON_VERSION} -m ensurepip \
    && python${PYTHON_VERSION} -m pip install --upgrade pip \
    && python${PYTHON_VERSION} -m pip install --upgrade awscli \
    && python${PYTHON_VERSION} -m pip install --upgrade --no-cache-dir "sglang[all]==${SGL_VERSION}"

# to be able to know the build versions at runtime
RUN echo "cuda version: ${CUDA_VERSION_DOT}" >> sglang_versions.txt \
    && echo "python version: ${PYTHON_VERSION}" >> sglang_versions.txt \
    && echo "torch version: ${TORCH_VERSION}" >> sglang_versions.txt \
    && echo "sglang version: ${SGL_VERSION}" >> sglang_versions.txt

COPY "extend/"${CUSTOMIZE_SGLANG} ${CUSTOMIZE_SGLANG}
COPY "extend/"${START_SGLANG} ${START_SGLANG}
RUN ls -lh
    # && bash ${CUSTOMIZE_SGLANG}

# turn needed build args into runtime env vars
# set up python version
ENV PYTHON_VERSION=${PYTHON_VERSION}
# communication parameters
ENV SGL_PORT=${SGL_PORT}
ENV SGL_HOST=${SGL_HOST}
# SGLang parameters
ENV SGL_TP_SIZE=${SGL_TP_SIZE}
ENV SGL_LOG_LEVEL=${SGL_LOG_LEVEL}
ENV SGL_PARAMS=""
# model info
ENV SGL_MODEL=""
ENV SGL_MODEL_DIR=${SGL_MODEL_DIR}

EXPOSE ${SGL_PORT}

# trying to avoid CUDA out-of-memory errors.
# see https://pytorch.org/docs/stable/notes/cuda.html
# also https://iamholumeedey007.medium.com/memory-management-using-pytorch-cuda-alloc-conf-dabe7adec130
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

CMD ["bash", "-c", "bash start_sglang.sh || sleep infinity"]

